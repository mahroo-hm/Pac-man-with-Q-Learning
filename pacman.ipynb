{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "Focp9MOTLpZe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set a default grid that is the question's grid. different grid pattern can be used in case of need."
      ],
      "metadata": {
        "id": "4_wUaoKEKrh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "defaultGrid = [['A','D','D','D','D','D','D','D','D'],\n",
        "               ['D','W','W','W','D','W','W','W','D'],\n",
        "               ['D','W','D','D','D','D','D','W','D'],\n",
        "               ['D','D','D','W','E','W','D','D','D'],\n",
        "               ['D','W','D','W','E','W','D','W','D'],\n",
        "               ['D','W','D','D','W','D','D','W','D'],\n",
        "               ['D','D','D','D','D','D','D','D','D']]\n",
        "prev_state = defaultGrid"
      ],
      "metadata": {
        "id": "pKk4AFgWG_z4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Environment**\n",
        "---\n",
        "Pacman env consists of 5 methods:\n",
        "\n",
        "1. init:\n",
        "> initialization of class's variables such as agents' configuration and number of total dots in the grid.\n",
        "2. valid_action_space:\n",
        "> every 4 possible actions are not always valid. this functions excludes movements that places the agent outside the grid (less that 0 and more that max number of total columns and rows) and those movements that lead to a wall which is indicated as 'W' in the grid.\n",
        "3. count_dots:\n",
        "> counts the dots. when equal to 0, the game is over.\n",
        "4. step:\n",
        "> checks if the game is over and takes an action (up, down, left, right) as an attribute transforms to the next state:\n",
        "*   places the agent 'A' based on current (x, y) and after applyting the direction\n",
        "*   set the prev place as 'E'\n",
        "*   set the reward: if there are any dots 'D' in the next square agent will recieve a +1 reward and also dots count will increase - if the next square is empty the agents receives no reward (+0), if agent reaches a ghost 'G' it will be given a -1 reward.\n",
        "5. reset: reset all parameters so that the agent can start training again."
      ],
      "metadata": {
        "id": "kFo1RYnOK6hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PacmanEnv():\n",
        "    def __init__(self, agent_x = 0, agent_y = 0, g = defaultGrid):\n",
        "        self.action_space = ['U', 'D', 'L', 'R']\n",
        "        self.n_action_space = len(self.action_space)\n",
        "\n",
        "        self.state = g\n",
        "        self.prev_state = g\n",
        "        self.x = agent_x\n",
        "        self.y = agent_y\n",
        "        self.originalGird, self.originalX, self.originalY = g, agent_x, agent_y\n",
        "\n",
        "        self.state_observation = [self.x, self.y]\n",
        "        self.max_r = len(self.state) - 1\n",
        "        self.max_c = len(self.state[0]) - 1\n",
        "\n",
        "        self.n_dots = self.count_dots()\n",
        "        self.done = False\n",
        "\n",
        "    def valid_action_space(self):\n",
        "        valid_actions = []\n",
        "        for action in self.action_space:\n",
        "            if action == 'U' and self.x != 0 and self.state[self.x - 1][self.y] != 'W':\n",
        "                valid_actions.append(action)\n",
        "            elif action == 'D' and self.x != self.max_r and self.state[self.x + 1][self.y] != 'W':\n",
        "                valid_actions.append(action)\n",
        "            elif action == 'L' and self.y != 0 and self.state[self.x][self.y - 1] != 'W':\n",
        "                valid_actions.append(action)\n",
        "            elif action == 'R' and self.y != self.max_c and self.state[self.x][self.y + 1] != 'W':\n",
        "                valid_actions.append(action)\n",
        "        return valid_actions\n",
        "\n",
        "    def count_dots(self):\n",
        "        n = 0\n",
        "        for i in range (self.max_r):\n",
        "            for j in range (self.max_c):\n",
        "                if self.state[i][j] == 'D':\n",
        "                    n += 1\n",
        "        return n\n",
        "\n",
        "    def step(self, action):\n",
        "        self.state[self.x][self.y] = 'E'\n",
        "        reward = 0\n",
        "\n",
        "        if action == 'U':\n",
        "            if self.state[self.x - 1][self.y] == 'G':\n",
        "                reward -= 1\n",
        "                self.done = True\n",
        "            elif self.state[self.x - 1][self.y] == 'D':\n",
        "                reward += 1\n",
        "                self.n_dots -= 1\n",
        "            self.x -= 1\n",
        "            self.state[self.x][self.y] = 'A'\n",
        "\n",
        "        elif action == 'D':\n",
        "            if self.state[self.x + 1][self.y] == 'G':\n",
        "                reward -= 1\n",
        "            elif self.state[self.x + 1][self.y] == 'D':\n",
        "                reward += 1\n",
        "                self.n_dots -= 1\n",
        "            self.x += 1\n",
        "            self.state[self.x][self.y] = 'A'\n",
        "\n",
        "        elif action == 'L':\n",
        "            if self.state[self.x][self.y - 1] == 'G':\n",
        "                reward -= 1\n",
        "            elif self.state[self.x][self.y - 1] == 'D':\n",
        "                reward += 1\n",
        "                self.n_dots -= 1\n",
        "            self.y -= 1\n",
        "            self.state[self.x][self.y] = 'A'\n",
        "\n",
        "        elif action == 'R':\n",
        "            if self.state[self.x][self.y + 1] == 'G':\n",
        "                reward -= 1\n",
        "            elif self.state[self.x][self.y + 1] == 'D':\n",
        "                reward += 1\n",
        "                self.n_dots -= 1\n",
        "            self.y += 1\n",
        "            self.state[self.x][self.y] = 'A'\n",
        "\n",
        "        done = True if self.n_dots <= 0 else False\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        self.done = False\n",
        "        self.state = defaultGrid\n",
        "        self.prev_state = defaultGrid\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        self.n_dots = self.count_dots()\n",
        "        self.state_observation = [self.x, self.y]\n",
        "        return self.state"
      ],
      "metadata": {
        "id": "uYVmWEZJD-F5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q agent**\n",
        "---\n",
        "primarily makes Q-table. the Q-table is a 2D dictionary which the first key is a tuple of (state, agent_x, agent_y) and the second key is \"action\" and the value is the Q-val. this structures ensured the uniqueness of the table but the rows are obtained as the model learns. The Q-table looks something like this for a hypothetical 3x3 grid:\n",
        "\n",
        "> qtable = {('ADW;DDD;DWE', 0, 0):{'U': 0, 'R': 0, 'L':0.5, 'R', 0.6}\n",
        "\n",
        "\n",
        "1. init:\n",
        "> initialization of class's variables.\n",
        "2. updateTable:\n",
        "> adds the new tuple along with action and corresponding q-val to the table. if the key does not exist, the state and all 4 actions are initialized and if the key already exists, it will be updated for the specific action key.\n",
        "3. getKeyTup:\n",
        "> together with \"encode\" and \"getAgentPos\" methods, form the tuple to act as key in qtable dict.\n",
        "4. encode:\n",
        "> encode the current grid matrix a.k.a state to a string of form: ow1; row2; row3; ...\n",
        "5. decode:\n",
        "> turn the string_state back to the 2D array form.\n",
        "6. getArgMax:\n",
        "> for a state, the possible actions are calculated and are stored in \"directions\". all valid actions which depends on current state are converted to the valid input form for the qtable (also those that the agent has not seen yet are added to the table and the value is initialized as 0). the method returns the max q-value.\n",
        "7. getAgentPos:\n",
        "> searches the grid to find the agent's position.\n",
        "8. getQval:\n",
        "> given the state and the action, this function returns the Q-value using the dict structure. first the state is converted to the tuple format and then the value is obtained: val = qtable[(state, x, y)][action]\n",
        "9. printQtable"
      ],
      "metadata": {
        "id": "Bke9tBGXOwp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentQ():\n",
        "    def __init__(self, grid):\n",
        "        self.qtable = {}\n",
        "        self.max_r = len(grid) - 1\n",
        "        self.max_c = len(grid[0]) - 1\n",
        "\n",
        "    def updateTable(self, state, action, qval):\n",
        "        x, y = self.getAgentPos(state)\n",
        "        stateTup = self.getKeyTup(state, x, y)\n",
        "\n",
        "        if stateTup not in self.qtable.keys():\n",
        "            self.qtable[stateTup] = {'U':0, 'D':0, 'L':0, 'R':0}\n",
        "        self.qtable[stateTup].update({action : qval})\n",
        "\n",
        "    def getKeyTup(self, state, x, y):\n",
        "        state_str = self.encode(state)\n",
        "        return (state_str, x, y)\n",
        "\n",
        "\n",
        "    def encode(self, state):\n",
        "        s = \"\"\n",
        "        n = len(state)\n",
        "        m = len(state[0])\n",
        "        for i in range(0, n):\n",
        "            for j in range(0, m):\n",
        "                s += state[i][j]\n",
        "            s += ';'\n",
        "        return s\n",
        "\n",
        "    def decode(self, str_state):\n",
        "        row = []\n",
        "        s = []\n",
        "        for i in str_state:\n",
        "            if i == ';':\n",
        "                s.append(row)\n",
        "                row = []\n",
        "            else:\n",
        "                row.append(i)\n",
        "        return s\n",
        "\n",
        "    def getArgMax(self, state, directions):\n",
        "        mx = \"\"\n",
        "        mx_val = -1000000\n",
        "        n = state\n",
        "        x, y = self.getAgentPos(state)\n",
        "        for dir in directions:\n",
        "            tup = ()\n",
        "            tmp = 0\n",
        "            if dir == 'U':\n",
        "                tup = self.getKeyTup(state, x - 1, y)\n",
        "            elif dir == 'D':\n",
        "                tup = self.getKeyTup(state, x + 1, y)\n",
        "            elif dir == 'R':\n",
        "                tup = self.getKeyTup(state, x, y + 1)\n",
        "            elif dir == 'L':\n",
        "                tup = self.getKeyTup(state, x, y - 1)\n",
        "\n",
        "            if tup not in self.qtable.keys():\n",
        "                self.qtable[tup] = {'U':0, 'D':0, 'L':0, 'R':0}\n",
        "                tmp = 0\n",
        "            else:\n",
        "                tmp = self.qtable[tup][dir]\n",
        "\n",
        "            if tmp >= mx_val:\n",
        "                mx_val = tmp\n",
        "                mx = dir\n",
        "        return [mx, mx_val]\n",
        "\n",
        "\n",
        "    def getAgentPos(self, state):\n",
        "        n = len(state)\n",
        "        m = len(state[0])\n",
        "        for i in range(n):\n",
        "            for j in range(m):\n",
        "                if state[i][j] == 'A':\n",
        "                    return i, j;\n",
        "\n",
        "    def getQval(self, state, action):\n",
        "        x, y = self.getAgentPos(state)\n",
        "        tup = self.getKeyTup(state, x, y)\n",
        "        if tup not in self.qtable.keys() and x >= 0 and y >= 0:\n",
        "            self.qtable[tup] = {'U':0, 'D':0, 'L':0, 'R':0}\n",
        "            return 0\n",
        "        else:\n",
        "            return self.qtable[tup][action]\n",
        "\n",
        "    def printQtable(self):\n",
        "        for state in self.qtable.keys():\n",
        "            print(state)\n",
        "            print(self.qtable[state])"
      ],
      "metadata": {
        "id": "5xyFcJAuB2xt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**\n",
        "--\n",
        "**gamma**: discounting rate, it make the future values less important than current state\n",
        "\n",
        "**alpha**: learning rate\n",
        "\n",
        "**epsilon**: treshold to exploration rate\n",
        "\n",
        "*for* **loop**\n",
        "\n",
        "\"exp_exp_tradeoff\" desides if the agent should choose the next action randomly or make the best decision based on q-values. the action is chosen and env.step executeed the action. now we are in the new state and qval is calculated using bellman equation. the value in the table is updated andwe proceed to the next state till we collect all dots."
      ],
      "metadata": {
        "id": "QWecnsZvUp_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = PacmanEnv()\n",
        "action_space_size = env.n_action_space\n",
        "qtable = AgentQ(env.state)\n",
        "\n",
        "total_episodes = 1\n",
        "alpha = 0.8\n",
        "max_steps = 2                # Max steps per episode\n",
        "gamma = 0.95                  # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability\n",
        "decay_rate = 0.005\n",
        "\n",
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    old_state = env.reset()\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    step = 0\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        step += 1\n",
        "        valid_actions = env.valid_action_space()\n",
        "\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = qtable.getArgMax(old_state, valid_actions)[0]\n",
        "        else:\n",
        "            action = valid_actions[random.randint(0, len(valid_actions) - 1)]\n",
        "\n",
        "\n",
        "        new_state, reward, done = env.step(action)\n",
        "\n",
        "        print(\"state:\")\n",
        "        print(prev_state)\n",
        "\n",
        "        print(\"valid actions\")\n",
        "        print(valid_actions)\n",
        "\n",
        "        print(\"action\")\n",
        "        print(action)\n",
        "\n",
        "        print(\"reward\")\n",
        "        print(reward)\n",
        "\n",
        "        print(\"new state:\")\n",
        "        print(new_state)\n",
        "\n",
        "        valid_actions_new = env.valid_action_space()\n",
        "\n",
        "        print(\"valid  newactions\")\n",
        "        print(valid_actions_new)\n",
        "\n",
        "\n",
        "        qval = (1 - alpha) * qtable.getQval(old_state, action) + alpha * (reward + gamma * qtable.getArgMax(new_state, valid_actions_new)[1])\n",
        "\n",
        "        print(\"qval\")\n",
        "        print(qval)\n",
        "        qtable.updateTable(old_state, action, qval)\n",
        "        print(\"qtable\")\n",
        "        qtable.printQtable()\n",
        "\n",
        "        total_rewards += reward\n",
        "\n",
        "        # Our new state is state\n",
        "        old_state = new_state\n",
        "\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
        "    rewards.append(total_rewards)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0rAAzf6UEBn",
        "outputId": "47df8673-3628-4ff8-b8ab-bbe78217b670"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state:\n",
            "[['E', 'A', 'D', 'D', 'D', 'D', 'D', 'D', 'D'], ['D', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D'], ['D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D'], ['D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D'], ['D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']]\n",
            "valid actions\n",
            "['D', 'R']\n",
            "action\n",
            "R\n",
            "reward\n",
            "1\n",
            "new state:\n",
            "[['E', 'A', 'D', 'D', 'D', 'D', 'D', 'D', 'D'], ['D', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D'], ['D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D'], ['D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D'], ['D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']]\n",
            "valid  newactions\n",
            "['L', 'R']\n",
            "qval\n",
            "0.8\n",
            "qtable\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0.8}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "state:\n",
            "[['E', 'E', 'A', 'D', 'D', 'D', 'D', 'D', 'D'], ['D', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D'], ['D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D'], ['D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D'], ['D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']]\n",
            "valid actions\n",
            "['L', 'R']\n",
            "action\n",
            "R\n",
            "reward\n",
            "1\n",
            "new state:\n",
            "[['E', 'E', 'A', 'D', 'D', 'D', 'D', 'D', 'D'], ['D', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D'], ['D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D'], ['D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D'], ['D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']]\n",
            "valid  newactions\n",
            "['L', 'R']\n",
            "qval\n",
            "0.8\n",
            "qtable\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0.8}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0.8}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 3)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "state:\n",
            "[['E', 'A', 'E', 'D', 'D', 'D', 'D', 'D', 'D'], ['D', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D'], ['D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D'], ['D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D'], ['D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']]\n",
            "valid actions\n",
            "['L', 'R']\n",
            "action\n",
            "L\n",
            "reward\n",
            "0\n",
            "new state:\n",
            "[['E', 'A', 'E', 'D', 'D', 'D', 'D', 'D', 'D'], ['D', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D'], ['D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D'], ['D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D'], ['D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']]\n",
            "valid  newactions\n",
            "['L', 'R']\n",
            "qval\n",
            "0.0\n",
            "qtable\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0.8}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0.8}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 3)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0.0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "state:\n",
            "[['A', 'E', 'E', 'D', 'D', 'D', 'D', 'D', 'D'], ['D', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D'], ['D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D'], ['D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D'], ['D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']]\n",
            "valid actions\n",
            "['L', 'R']\n",
            "action\n",
            "L\n",
            "reward\n",
            "0\n",
            "new state:\n",
            "[['A', 'E', 'E', 'D', 'D', 'D', 'D', 'D', 'D'], ['D', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D'], ['D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D'], ['D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D'], ['D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']]\n",
            "valid  newactions\n",
            "['D', 'R']\n",
            "qval\n",
            "0.0\n",
            "qtable\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0.8}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0.8}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 3)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0.0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('AEEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0.0, 'R': 0}\n",
            "('AEEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 1, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('AEEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "state:\n",
            "[['E', 'E', 'E', 'D', 'D', 'D', 'D', 'D', 'D'], ['A', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D'], ['D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D'], ['D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D'], ['D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']]\n",
            "valid actions\n",
            "['D', 'R']\n",
            "action\n",
            "D\n",
            "reward\n",
            "1\n",
            "new state:\n",
            "[['E', 'E', 'E', 'D', 'D', 'D', 'D', 'D', 'D'], ['A', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D'], ['D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D'], ['D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D'], ['D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D'], ['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']]\n",
            "valid  newactions\n",
            "['U', 'D']\n",
            "qval\n",
            "0.8\n",
            "qtable\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0.8}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0.8}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 3)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0.0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('AEEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0.0, 'R': 0}\n",
            "('AEEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 1, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('AEEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEEDDDDDD;AWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 1, 0)\n",
            "{'U': 0, 'D': 0.8, 'L': 0, 'R': 0}\n",
            "('EEEDDDDDD;AWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEEDDDDDD;AWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 2, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qtable.printQtable()\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmmrNguYuWEM",
        "outputId": "fe041ec1-26e7-424b-eadd-6095b9f93735"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0.8}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EADDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0.8}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEADDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 3)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0.0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EAEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 2)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('AEEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0.0, 'R': 0}\n",
            "('AEEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 1, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('AEEDDDDDD;DWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 1)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEEDDDDDD;AWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 1, 0)\n",
            "{'U': 0, 'D': 0.8, 'L': 0, 'R': 0}\n",
            "('EEEDDDDDD;AWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 0, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "('EEEDDDDDD;AWWWDWWWD;DWDDDDDWD;DDDWEWDDD;DWDWEWDWD;DWDDWDDWD;DDDDDDDDD;', 2, 0)\n",
            "{'U': 0, 'D': 0, 'L': 0, 'R': 0}\n",
            "Score over time: 3.0\n",
            "[3]\n"
          ]
        }
      ]
    }
  ]
}